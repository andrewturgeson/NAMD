#! /bin/bash
#$ -S /bin/bash

# Replace <CASENAME> with the name of the case you're running. Do this below
# too where the std{out,err} files are defined.
#$ -N PME.A.0
#$ -V

# Here, replace <NPHYSICALNODES> with the number of compute nodes you want to
# run the job on.
#$ -pe openmpi_ib 1

#$ -cwd
#$ -e recent.stderr
#$ -o recent.stdout


source /etc/profile.d/modules.sh
module load shared
module load cuda10.2/toolkit/10.2
module load namd/2.13-current/2.13
module load openmpi/gcc/64/3.1.0

echo "###########################################################################"
#echo " pe_hostfile: $PE_HOSTFILE"
#cat $PE_HOSTFILE
echo " NAMD got $NSLOTS slots across $NHOSTS machines"
echo " Custom namd machines file:"
cat $TMPDIR/namd-machines
echo "###########################################################################"

# This line represents how many total threads you want to run; this should be
# equal to <NPHYSCALNODES> multiplied by 27. (There are 28 cores per compute
# node, but one core per node needs to be set aside for communication.
TOTALNTHREADS=27

# This sets the number of threads per process; as I understand it currently,
# this should always be 27 when running multi-node SMP jobs with a single
# process per physical node (again, reserving one thread for communication).
THREADSPERPROCESS=27

# Don't forget to replace <INPUTFILE> with the name of your namd parameter file.
cmd="charmrun ++mpiexec ++remote-shell `which mpiexec` +p $TOTALNTHREADS ++ppn $THREADSPERPROCESS `which namd2` +idlepoll +setcpuaffinity s0.inp > s0.log"

echo "Starting NAMD case on `date`"
$cmd
echo "NAMD case complete on `date`"
